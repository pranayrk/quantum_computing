## What is quantum computing?

* The computers we use today rely on classical information theory, which are based on 'bits' which can represents a 0 or 1 state. 
* These computers, referred to as classical computers henceforth, can be reduced to an equivalence with a Turing Machine, i.e. they can both compute similar things with similar efficiency.
* In the last decades of the twentieth century, certain scientists sought to combine two of the recent most influential and revolutionary theories: information theory and quantum mechanics.
* They recognized that certain quantum phenomena (those associated with "entangled particles") could not be simulated efficiently by a Turing machine.
* This observation led to speculation that these quantum phenomena could be used to speed up computation in general.
* Such a program required rethinking the information theoretic model underlying computation, which resulted in a new framework for computation called quantum computing

## A note on Dirac's Bra/Ket Notation

The Bra–ket notation, or Dirac notation, is a notation for linear algebra and linear operators on complex vector spaces.
It is specifically designed to ease the types of calculations that frequently come up in quantum mechanics, and it's use is very widespread to denote quantum states and linear transformations that act on these quantum stat to denote quantum states and linear transformations that act on these quantum states.
We will be using it extensively in these notes and will incrementally explain how it works.

## From Quantum Computing Since Democritus, Scott Aaronson

"Quantum mechanics is a beautiful generalization of the laws of probability: a generalization based on the 2-norm rather than the 1-norm, and on complex numbers rather than nonnegative real numbers. It can be studied completely separately from its applications to physics (and indeed, doing so provides a good starting point for learning the physical applications later). This generalized probability theory leads naturally to a new model of computation – the quantum computing model – that challenges ideas about computation once considered a priori, and that theoretical computer scientists might have been driven to invent for their own purposes, even if there were no relation to physics. In short, while quantum mechanics was invented a century ago to solve technical problems in physics, today it can be fruitfully explained from an extremely different perspective: as part of the history of ideas, in math, logic, computation, and philosophy, about the limits of the knowable."

## From Carnegie Mellon -- Quantum Computation and Information 2015
The idea of quantum computation was pioneered in the 1980s mainly by Feynman and Deutsch, with Albert independently introducing quantumautomata and with Benioff analyzing the link between quantum mechanics andreversible classical computation. The initial idea of Feynman was the following: Although it is perfectly possible to use a (normal) computer to simulate the behavior of n-particlesystems evolving according to the laws of quantum, it seems be extremely inefficient. In particular, it seems to take an amount of time/space that is exponential in $n$. This is peculiar because the actual particles can be viewed as simulating themselves efficiently. So why not call the particles themselves a “computer”? After all, although we have sophisticated theoretical models of (normal) computation, in the end computers are ultimately physical objects operating according to the laws of physics. If we simply regard the particles following their natural quantum-mechanical behavior as a computer, then this “quantum computer” appears to be performing a certain computation (namely, simulating a quantum system)exponentially more efficiently than we know how to perform it with a normal, “classical” computer. Perhaps we can carefully engineer multi-particle systems in such a way that their natural quantum behavior will do other interesting computations exponentially more efficiently than classical computers can.This is the basic idea behind quantum computers. As it turns out, you can get (seemingly) exponential speedups for a (seemingly) small number of natural computational problems by carefully designing a multi-particle quantum system and letting it evolve according tothe (100-year old, extremely well-confirmed) laws of quantum mechanics. By far the most spectacular example is Shor’s factoring algorithm, an algorithm implementable on a quantum computer that can factor any $n$-digit integer (with high probability) in roughly $n^2$-time. This is contrast to the fact that the fastest known “classical” algorithm for factoring $n$-digit integers seems to require roughly $2^{n^{1/3}}$-time, and in fact the presumed computational difficulty of factoring is relied upon in an enormous number of real-world cryptographic applications
